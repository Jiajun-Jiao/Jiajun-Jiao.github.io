---
title: "SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark"
collection: publications
permalink: /publication/2024-02-08-paper-title-number-2
date: 2024-02-08
venue: 'ACL 2024'
paperurl: 'https://arxiv.org/abs/2402.05138'
citation: 
---
_Zhenwen Liang_, _Kehan Guo_, _Gang Liu_, _Taicheng Guo_, _Yujun Zhou_, _Tianyu Yang_,  **Jiajun Jiao**, _Renjie Pi_, _Jipeng Zhang_, _Xiangliang Zhang_

The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models. Our benchmark and analysis will be available at [this https URL](https://scemqa.github.io/)

[Visit paper here](https://arxiv.org/abs/2402.05138)

